# HÆ°á»›ng dáº«n Tá»‘i Æ¯u Training - 100 Epochs trong 8 giá»

## ğŸ¯ Má»¥c tiÃªu

- **Kaggle (2x T4)**: Train 100 epochs trong 8 giá»
- **Local (RTX 3070 8GB)**: Train vÃ i epoch Ä‘á»ƒ test cháº¥t lÆ°á»£ng vÃ  kháº£ nÄƒng há»c
- **Dataset**: Chá»‰ CROHME (~3,332 samples)

## ğŸ“Š TÃ­nh toÃ¡n Thá»i gian

### Kaggle (2x T4, 16GB má»—i GPU)
- **Batch size**: 4 per GPU Ã— 2 GPU = 8 total
- **Gradient accumulation**: 4
- **Effective batch size**: 4 Ã— 2 Ã— 4 = **32**
- **Steps per epoch**: 3,332 / 32 = **~104 steps**
- **100 epochs**: 104 Ã— 100 = **10,400 steps**
- **Thá»i gian cáº§n**: 8 giá» = 28,800 giÃ¢y
- **Cáº§n Ä‘áº¡t**: **2.77 giÃ¢y/step** âœ… (Kháº£ thi vá»›i T4!)

### Local (RTX 3070, 8GB)
- **Batch size**: 2
- **Gradient accumulation**: 8
- **Effective batch size**: 2 Ã— 8 = **16**
- **Steps per epoch**: 3,332 / 16 = **~208 steps**
- **5 epochs**: 208 Ã— 5 = **1,040 steps**
- **Thá»i gian Æ°á»›c tÃ­nh**: **30-60 phÃºt** (Ä‘á»§ Ä‘á»ƒ test)

## âš™ï¸ CÃ¡c Tá»‘i Æ¯u ÄÃ£ Ãp Dá»¥ng

### 1. Giáº£m Image Resolution
- **TrÆ°á»›c**: `image_max_pixels: 65536`
- **Sau**: `image_max_pixels: 32768` (giáº£m 50%)
- **LÃ½ do**: TÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½, váº«n Ä‘á»§ cháº¥t lÆ°á»£ng cho cÃ´ng thá»©c toÃ¡n

### 2. Giáº£m LoRA Rank
- **TrÆ°á»›c**: `lora_rank: 16`, `lora_alpha: 32`
- **Sau**: `lora_rank: 8`, `lora_alpha: 16` (giáº£m 50% tham sá»‘)
- **LÃ½ do**: Giáº£m sá»‘ tham sá»‘ train â†’ tÄƒng tá»‘c Ä‘á»™ backward pass

### 3. Giáº£m Sequence Length
- **TrÆ°á»›c**: `cutoff_len: 2048`
- **Sau**: `cutoff_len: 1024` (giáº£m 50%)
- **LÃ½ do**: LaTeX cÃ´ng thá»©c thÆ°á»ng ngáº¯n, 1024 Ä‘á»§ dÃ¹ng

### 4. Giáº£m Workers
- **TrÆ°á»›c**: `preprocessing_num_workers: 4`, `dataloader_num_workers: 2`
- **Sau**: `preprocessing_num_workers: 2`, `dataloader_num_workers: 1`
- **LÃ½ do**: Giáº£m overhead, tÄƒng tá»‘c Ä‘á»™ I/O

### 5. TÄƒng Batch Size (Kaggle)
- **Kaggle**: `per_device_train_batch_size: 4` (2 GPU â†’ 8 total)
- **Local**: `per_device_train_batch_size: 2`
- **LÃ½ do**: Táº­n dá»¥ng VRAM, giáº£m sá»‘ steps

### 6. Giáº£m Save Frequency
- **TrÆ°á»›c**: `save_steps: 500`
- **Sau**: `save_steps: 200`
- **LÃ½ do**: Giáº£m I/O overhead (váº«n Ä‘á»§ Ä‘á»ƒ monitor)

## ğŸ“ Cáº¥u trÃºc Files

```
BaseLine/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Uni-MuMER-train-local.yaml    # Config cho RTX 3070 (test)
â”‚   â”œâ”€â”€ Uni-MuMER-train-kaggle.yaml   # Config cho Kaggle (100 epochs)
â”‚   â””â”€â”€ LLaMA-Factory/
â”œâ”€â”€ train_local.sh                    # Script train local
â”œâ”€â”€ train_kaggle.sh                    # Script train Kaggle
â””â”€â”€ HUONG_DAN_TOI_UU_TRAIN.md         # File nÃ y
```

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. Train trÃªn Local (RTX 3070) - Test

```bash
cd /home/nhat/Uni-MuMER-project/BaseLine
conda activate unimumer
bash train_local.sh
```

**Káº¿t quáº£ mong Ä‘á»£i**:
- Train 5 epochs trong 30-60 phÃºt
- Kiá»ƒm tra loss giáº£m dáº§n
- Kiá»ƒm tra accuracy/metrics tÄƒng trong vÃ i epoch Ä‘áº§u
- Náº¿u OK â†’ chuyá»ƒn sang Kaggle

### 2. Train trÃªn Kaggle (2x T4) - Full Training

#### BÆ°á»›c 1: Upload code lÃªn Kaggle
```bash
# Push code lÃªn GitHub
git add .
git commit -m "Optimized config for 100 epochs in 8 hours"
git push
```

#### BÆ°á»›c 2: Táº¡o Kaggle Notebook
1. Táº¡o notebook má»›i trÃªn Kaggle
2. Enable **2x T4 GPU** (P100 hoáº·c T4 x2)
3. Clone repo hoáº·c upload code

#### BÆ°á»›c 3: Cháº¡y training
```python
# Trong Kaggle notebook
!bash train_kaggle.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:
```python
import os
os.chdir('/kaggle/working/Uni-MuMER-project/BaseLine/train/LLaMA-Factory')
!torchrun --nproc_per_node=2 --master_port=29500 \
    -m llamafactory.cli train ../Uni-MuMER-train-kaggle.yaml
```

## ğŸ“ˆ Monitor Training

### TensorBoard
```bash
# Local
tensorboard --logdir saves/qwen2.5_vl-3b/lora/sft/standred/uni-mumer_crohme_local/logs/

# Kaggle (náº¿u cÃ³ thá»ƒ)
tensorboard --logdir saves/qwen2.5_vl-3b/lora/sft/standred/uni-mumer_crohme_kaggle/logs/
```

### Kiá»ƒm tra Logs
- Loss giáº£m dáº§n theo epochs
- Learning rate theo cosine schedule
- Checkpoints Ä‘Æ°á»£c lÆ°u má»—i 200 steps

## ğŸ”§ TÃ¹y Chá»‰nh Náº¿u Cáº§n

### Náº¿u Kaggle cháº¡y quÃ¡ cháº­m (>8 giá»)
1. **Giáº£m thÃªm `image_max_pixels`**: xuá»‘ng `16384` hoáº·c `8192`
2. **TÄƒng `per_device_train_batch_size`**: lÃªn `6` hoáº·c `8` (náº¿u cÃ²n VRAM)
3. **Giáº£m `gradient_accumulation_steps`**: xuá»‘ng `2` hoáº·c `3`

### Náº¿u Local bá»‹ OOM
1. **Giáº£m `per_device_train_batch_size`**: vá» `1`
2. **TÄƒng `gradient_accumulation_steps`**: lÃªn `16`
3. **Giáº£m `image_max_pixels`**: xuá»‘ng `16384`

### Náº¿u muá»‘n train nhanh hÆ¡n Ä‘á»ƒ test
1. **Giáº£m `num_train_epochs`**: xuá»‘ng `1` hoáº·c `2`
2. **Giáº£m `save_steps`**: xuá»‘ng `100` (Ã­t I/O hÆ¡n)

## ğŸ“Š So SÃ¡nh Config

| Tham sá»‘ | Local (RTX 3070) | Kaggle (2x T4) |
|---------|------------------|----------------|
| `image_max_pixels` | 32768 | 32768 |
| `lora_rank` | 8 | 8 |
| `cutoff_len` | 1024 | 1024 |
| `per_device_train_batch_size` | 2 | 4 |
| `gradient_accumulation_steps` | 8 | 4 |
| `num_train_epochs` | 5 | 100 |
| `quantization_bit` | 4 | 4 |
| **Effective batch** | 16 | 32 |
| **Steps/epoch** | ~208 | ~104 |
| **Thá»i gian** | 30-60 phÃºt | ~8 giá» |

## âœ… Checklist

- [x] Config local tá»‘i Æ°u cho RTX 3070
- [x] Config Kaggle tá»‘i Æ°u cho 2x T4
- [x] Script train cho cáº£ 2 mÃ´i trÆ°á»ng
- [x] TÃ­nh toÃ¡n thá»i gian chÃ­nh xÃ¡c
- [x] Giáº£m tham sá»‘ Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™
- [x] Giá»¯ cháº¥t lÆ°á»£ng mÃ´ hÃ¬nh (LoRA rank 8 váº«n tá»‘t)

## ğŸ“ LÆ°u Ã

1. **LoRA rank 8** váº«n Ä‘á»§ tá»‘t cho fine-tuning, khÃ´ng cáº§n rank 16
2. **Image 32768 pixels** váº«n Ä‘á»§ Ä‘á»ƒ nháº­n diá»‡n cÃ´ng thá»©c toÃ¡n
3. **Sequence length 1024** Ä‘á»§ cho LaTeX (thÆ°á»ng <500 tokens)
4. **Batch size lá»›n hÆ¡n** â†’ Ã­t steps hÆ¡n â†’ nhanh hÆ¡n
5. **Multi-GPU** tá»± Ä‘á»™ng chia batch, khÃ´ng cáº§n config thÃªm

## ğŸ› Troubleshooting

### Kaggle: "CUDA out of memory"
- Giáº£m `per_device_train_batch_size` xuá»‘ng `3` hoáº·c `2`
- TÄƒng `gradient_accumulation_steps` lÃªn `6` hoáº·c `8`

### Kaggle: "Training quÃ¡ cháº­m"
- Kiá»ƒm tra GPU utilization: `nvidia-smi`
- Giáº£m `preprocessing_num_workers` xuá»‘ng `1`
- TÄƒng `per_device_train_batch_size` náº¿u cÃ²n VRAM

### Local: "OOM error"
- Giáº£m `per_device_train_batch_size` vá» `1`
- Giáº£m `image_max_pixels` xuá»‘ng `16384`

---

**ChÃºc báº¡n train thÃ nh cÃ´ng! ğŸš€**

