# ğŸš€ Uni-MuMER Baseline - Cáº£i Tiáº¿n Hiá»‡u Suáº¥t Training

[![Uni-MuMER](https://img.shields.io/badge/Uni--MuMER-NeurIPS'25%20Spotlight-red)](https://github.com/BFlameSwift/Uni-MuMER)
[![LoRA](https://img.shields.io/badge/LoRA-4bit%20Quantization-green)](https://arxiv.org/abs/2106.09685)
[![GPU](https://img.shields.io/badge/GPU-RTX%203070%208GB-blue)]()

> **Fork vÃ  cáº£i tiáº¿n tá»« [Uni-MuMER](https://github.com/BFlameSwift/Uni-MuMER)** - Tá»‘i Æ°u Ä‘á»ƒ train trÃªn GPU nhá» vÃ  cáº£i thiá»‡n monitoring

## ğŸ“‹ Tá»•ng Quan

Repository nÃ y lÃ  má»™t **baseline cáº£i tiáº¿n** cá»§a [Uni-MuMER](https://github.com/BFlameSwift/Uni-MuMER) vá»›i cÃ¡c má»¥c tiÃªu:

- âœ… **Giáº£m yÃªu cáº§u tÃ i nguyÃªn**: Train trÃªn GPU 8GB (RTX 3070) thay vÃ¬ GPU lá»›n
- âœ… **TÄƒng tá»‘c Ä‘á»™**: Äáº¡t 100 epochs trong 8 giá» trÃªn Kaggle (2x T4)
- âœ… **Cáº£i thiá»‡n monitoring**: Hiá»ƒn thá»‹ accuracy trá»±c tiáº¿p trong terminal
- âœ… **Tá»‘i Æ°u workflow**: Scripts tá»± Ä‘á»™ng vÃ  hÆ°á»›ng dáº«n chi tiáº¿t

## ğŸ¯ So SÃ¡nh vá»›i Repository Gá»‘c

| TÃ­nh nÄƒng | Repository Gá»‘c | Baseline Cáº£i Tiáº¿n |
|-----------|----------------|-------------------|
| **PhÆ°Æ¡ng phÃ¡p** | Full Fine-tuning | **LoRA + 4-bit Quantization** |
| **Memory yÃªu cáº§u** | ~25GB VRAM | **~4-6GB VRAM** âœ… |
| **GPU tá»‘i thiá»ƒu** | A100/H100 | **RTX 3070 (8GB)** âœ… |
| **Tham sá»‘ train** | 100% (3B params) | **~1-2% (vÃ i triá»‡u params)** âœ… |
| **Monitoring** | Chá»‰ eval_loss | **Accuracy + Epoch Summary** âœ… |
| **Tá»‘c Ä‘á»™** | ChÆ°a tá»‘i Æ°u | **100 epochs/8 giá»** âœ… |
| **Scripts** | CÆ¡ báº£n | **Tá»± Ä‘á»™ng hÃ³a Ä‘áº§y Ä‘á»§** âœ… |

## ğŸš€ TÃ­nh NÄƒng ChÃ­nh

### 1. LoRA + 4-bit Quantization

Chuyá»ƒn Ä‘á»•i tá»« Full Fine-tuning sang **LoRA (Low-Rank Adaptation) + 4-bit Quantization**:

- **Giáº£m 80% memory**: Tá»« ~25GB xuá»‘ng ~4-6GB
- **Giáº£m 98% tham sá»‘ train**: Chá»‰ train adapter layers
- **TÄƒng tá»‘c Ä‘á»™**: Nhanh hÆ¡n do Ã­t tham sá»‘ cáº§n update
- **Giá»¯ cháº¥t lÆ°á»£ng**: LoRA rank 8 váº«n Ä‘áº¡t hiá»‡u suáº¥t tá»‘t

```yaml
finetuning_type: lora
lora_rank: 8
quantization_bit: 4
quantization_method: bnb
```

### 2. Tá»‘i Æ¯u Tá»‘c Äá»™ Training

**Má»¥c tiÃªu**: Train 100 epochs trong 8 giá» trÃªn Kaggle (2x T4)

**CÃ¡c tá»‘i Æ°u**:
- Giáº£m image resolution: `32768 pixels` (giáº£m 50%)
- Giáº£m LoRA rank: `8` (giáº£m 50% tham sá»‘)
- Giáº£m sequence length: `1024` (giáº£m 50%)
- Tá»‘i Æ°u batch size: Effective batch = 32 (Kaggle)

**Káº¿t quáº£**: âœ… Äáº¡t má»¥c tiÃªu!

### 3. Custom HMER Accuracy Metric

Táº¡o metric riÃªng cho HMER Ä‘á»ƒ hiá»ƒn thá»‹ **Exact Match Rate** trá»±c tiáº¿p trong training logs:

```python
# Tá»± Ä‘á»™ng hiá»ƒn thá»‹ trong logs
{'eval_loss': 0.045, 'hmer_exact_match_rate': 0.7234, 'hmer_avg_edit_distance': 0.1234}
```

**Lá»£i Ã­ch**:
- Theo dÃµi accuracy real-time
- KhÃ´ng cáº§n cháº¡y inference riÃªng
- Dá»… so sÃ¡nh giá»¯a cÃ¡c epochs

### 4. Epoch Display Callback

Hiá»ƒn thá»‹ káº¿t quáº£ Ä‘áº¹p máº¯t sau má»—i epoch:

```
================================================================================
                            EPOCH 1.00 SUMMARY                             
================================================================================

ğŸ“Š Training Metrics:
  â€¢ Loss:              0.0573
  â€¢ Learning Rate:     0.000381692

ğŸ“ˆ Evaluation Metrics:
  â€¢ Eval Loss:         0.0450
  â€¢ Eval Perplexity:   1.0460

ğŸ¯ HMER Accuracy:
  â€¢ Exact Match Rate:  72.34% (0.7234)
  â€¢ Avg Edit Distance: 0.1234
================================================================================
```

### 5. Scripts Há»— Trá»£

- **`train_local.sh`**: Script train trÃªn local vá»›i kiá»ƒm tra Ä‘áº§y Ä‘á»§
- **`train_kaggle.sh`**: Script train trÃªn Kaggle vá»›i multi-GPU
- **`scripts/calculate_accuracy.py`**: TÃ­nh accuracy tá»« checkpoint
- **`scripts/summarize_epoch_results.py`**: Tá»•ng há»£p káº¿t quáº£ tá»« táº¥t cáº£ epochs

## ğŸ“¦ CÃ i Äáº·t

### Quick Start

```bash
# 1. Clone repository
git clone https://github.com/YOUR_USERNAME/Uni-MuMER-project.git
cd Uni-MuMER-project/BaseLine

# 2. Setup mÃ´i trÆ°á»ng
bash setup_conda_local.sh

# 3. Giáº£i nÃ©n dá»¯ liá»‡u
unzip data.zip -d .

# 4. Train
conda activate unimumer
bash train_local.sh
```

### YÃªu Cáº§u

- **GPU**: RTX 3070 (8GB) hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng
- **RAM**: 16GB+ (khuyáº¿n nghá»‹ 32GB+)
- **Conda**: Miniconda hoáº·c Anaconda
- **CUDA**: 12.1+

## ğŸ‹ï¸ Training

### Local (RTX 3070)

```bash
conda activate unimumer
bash train_local.sh
```

**Config**: `train/Uni-MuMER-train-local.yaml`
- 5 epochs Ä‘á»ƒ test
- Batch size: 2, Gradient accumulation: 8
- LoRA rank: 8, Quantization: 4-bit

### Kaggle (2x T4)

```bash
bash train_kaggle.sh
```

**Config**: `train/Uni-MuMER-train-kaggle.yaml`
- 100 epochs (má»¥c tiÃªu: 8 giá»)
- Batch size: 4 per GPU Ã— 2 GPU
- Multi-GPU training tá»± Ä‘á»™ng

## ğŸ“Š Monitoring

### Xem Accuracy trong Terminal

Accuracy tá»± Ä‘á»™ng hiá»ƒn thá»‹ khi training vá»›i `compute_hmer_accuracy: true`:

```
{'hmer_exact_match_rate': 0.7234, 'hmer_accuracy': 0.7234, 'hmer_avg_edit_distance': 0.1234}
```

### Tá»•ng Há»£p Káº¿t Quáº£

```bash
# Tá»•ng há»£p táº¥t cáº£ epochs
python scripts/summarize_epoch_results.py saves/.../uni-mumer_crohme_local

# TÃ­nh accuracy tá»« checkpoint
python scripts/calculate_accuracy.py saves/.../checkpoint-400
```

## ğŸ“ˆ Káº¿t Quáº£

### Hiá»‡u Suáº¥t Training

| Metric | Gá»‘c | Cáº£i Tiáº¿n |
|--------|-----|----------|
| **Memory** | ~25GB | **~4-6GB** âœ… |
| **Tham sá»‘ train** | 3B (100%) | **VÃ i triá»‡u (1-2%)** âœ… |
| **Tá»‘c Ä‘á»™** | ChÆ°a tá»‘i Æ°u | **100 epochs/8h** âœ… |
| **GPU yÃªu cáº§u** | A100/H100 | **RTX 3070** âœ… |

### Monitoring

- âœ… Accuracy hiá»ƒn thá»‹ trá»±c tiáº¿p trong logs
- âœ… Epoch summary Ä‘áº¹p máº¯t, dá»… Ä‘á»c
- âœ… Scripts tá»± Ä‘á»™ng tÃ­nh vÃ  tá»•ng há»£p
- âœ… Dá»… phÃ¡t hiá»‡n overfitting/underfitting

## ğŸ“š TÃ i Liá»‡u

- **`HUONG_DAN_TRAIN.md`**: HÆ°á»›ng dáº«n training Ä‘áº§y Ä‘á»§
- **`HUONG_DAN_ACCURACY.md`**: HÆ°á»›ng dáº«n theo dÃµi accuracy
- **`QUICK_START.md`**: HÆ°á»›ng dáº«n nhanh
- **`CONDA_COMMANDS.md`**: Lá»‡nh Conda tham kháº£o

## ğŸ”§ Cáº¥u TrÃºc Project

```
BaseLine/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ Uni-MuMER-train-local.yaml    # Config cho RTX 3070
â”‚   â”œâ”€â”€ Uni-MuMER-train-kaggle.yaml  # Config cho Kaggle 2x T4
â”‚   â””â”€â”€ LLaMA-Factory/                # Training framework (modified)
â”‚       â””â”€â”€ src/llamafactory/train/
â”‚           â”œâ”€â”€ sft/
â”‚           â”‚   â”œâ”€â”€ hmer_metric.py           # Custom HMER accuracy metric
â”‚           â”‚   â””â”€â”€ workflow.py              # Modified Ä‘á»ƒ tÃ­ch há»£p metric
â”‚           â””â”€â”€ callbacks_epoch_display.py   # Epoch display callback
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ calculate_accuracy.py         # TÃ­nh accuracy tá»« checkpoint
â”‚   â”œâ”€â”€ summarize_epoch_results.py    # Tá»•ng há»£p káº¿t quáº£ epochs
â”‚   â””â”€â”€ eval_metrics_calculator.py    # Evaluation metrics
â”œâ”€â”€ train_local.sh                    # Script train local
â”œâ”€â”€ train_kaggle.sh                   # Script train Kaggle
â””â”€â”€ data/                             # Datasets (CROHME, HME100K, ...)
```

## ğŸ¯ Má»¥c TiÃªu Cáº£i Tiáº¿n

1. âœ… **Giáº£m yÃªu cáº§u tÃ i nguyÃªn**: Train trÃªn GPU 8GB
2. âœ… **TÄƒng tá»‘c Ä‘á»™**: 100 epochs trong 8 giá»
3. âœ… **Cáº£i thiá»‡n monitoring**: Accuracy real-time
4. âœ… **TÄƒng tÃ­nh tiá»‡n dá»¥ng**: Scripts tá»± Ä‘á»™ng, hÆ°á»›ng dáº«n chi tiáº¿t

## ğŸ“ Citation

Náº¿u sá»­ dá»¥ng code nÃ y, vui lÃ²ng cite cáº£ repository gá»‘c vÃ  cáº£i tiáº¿n:

```bibtex
@article{li2025unimumer,
  title = {Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition},
  author = {Li, Yu and Jiang, Jin and Zhu, Jianhua and Peng, Shuai and Wei, Baole and Zhou, Yuxuan and Gao, Liangcai},
  year = {2025},
  journal={arXiv preprint arXiv:2505.23566},
}
```

## ğŸ™ Acknowledgements

- **Repository gá»‘c**: [BFlameSwift/Uni-MuMER](https://github.com/BFlameSwift/Uni-MuMER)
- **LLaMA-Factory**: [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)
- **LoRA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

## ğŸ“„ License

Apache-2.0 License (giá»‘ng repository gá»‘c)

---

**â­ Náº¿u project nÃ y há»¯u Ã­ch, hÃ£y star repository!**

**ğŸ“§ LiÃªn há»‡**: [Email cá»§a báº¡n]

**ğŸ”— Repository gá»‘c**: https://github.com/BFlameSwift/Uni-MuMER

