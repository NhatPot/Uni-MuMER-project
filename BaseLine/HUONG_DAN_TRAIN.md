# ğŸ“š HÆ°á»›ng Dáº«n Train Uni-MuMER

HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ train mÃ´ hÃ¬nh Uni-MuMER trÃªn mÃ¡y local vÃ  Kaggle.

## ğŸ“‹ Má»¥c Lá»¥c

1. [Setup MÃ´i TrÆ°á»ng](#setup-mÃ´i-trÆ°á»ng)
2. [Chuáº©n Bá»‹ Dá»¯ Liá»‡u](#chuáº©n-bá»‹-dá»¯-liá»‡u)
3. [Training vá»›i LoRA + Quantization](#training-vá»›i-lora--quantization)
4. [Train Chá»‰ vá»›i CROHME](#train-chá»‰-vá»›i-crohme)
5. [Tá»‘i Æ¯u Training (100 Epochs trong 8 giá»)](#tá»‘i-Æ°u-training)
6. [Hiá»ƒn Thá»‹ Káº¿t Quáº£ Epoch](#hiá»ƒn-thá»‹-káº¿t-quáº£-epoch)
7. [Train trÃªn Kaggle](#train-trÃªn-kaggle)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ–¥ï¸ Cáº¥u HÃ¬nh MÃ¡y

- **CPU**: Intel E5-2680v4
- **GPU**: NVIDIA RTX 3070 (8GB VRAM)
- **RAM**: 144GB
- **OS**: Linux

---

## ğŸ“‹ Setup MÃ´i TrÆ°á»ng

### CÃ¡ch 1: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

```bash
cd "/home/nhat/Uni-MuMER-project/BaseLine"
bash setup_conda_local.sh
```

Script nÃ y sáº½:
- Táº¡o mÃ´i trÆ°á»ng conda `unimumer` vá»›i Python 3.10
- CÃ i Ä‘áº·t PyTorch vá»›i CUDA 12.1
- CÃ i Ä‘áº·t táº¥t cáº£ dependencies
- Clone vÃ  cÃ i Ä‘áº·t LLaMA-Factory

### CÃ¡ch 2: Thá»§ cÃ´ng

Xem `CONDA_COMMANDS.md` Ä‘á»ƒ biáº¿t cÃ¡c lá»‡nh conda chi tiáº¿t.

---

## ğŸ“¦ Chuáº©n Bá»‹ Dá»¯ Liá»‡u

1. **Giáº£i nÃ©n dá»¯ liá»‡u**:
   ```bash
   cd "/home/nhat/Uni-MuMER-project/BaseLine"
   unzip data.zip -d .
   ```

2. **Kiá»ƒm tra cáº¥u trÃºc**:
   ```
   data/
   â”œâ”€â”€ CROHME/
   â”œâ”€â”€ CROHME2023/
   â”œâ”€â”€ HME100K/
   â”œâ”€â”€ Im2LaTeXv2/
   â”œâ”€â”€ MathWriting/
   â””â”€â”€ MNE/
   ```

---

## ğŸ‹ï¸ Training vá»›i LoRA + Quantization

### Táº¡i sao dÃ¹ng LoRA + Quantization?

| PhÆ°Æ¡ng phÃ¡p | Memory cáº§n | Tham sá»‘ train |
|------------|------------|---------------|
| Full Fine-tuning | ~25GB | 100% (3B params) |
| LoRA | ~8-10GB | ~1-2% (vÃ i triá»‡u params) |
| **LoRA + 4-bit Quantization** | **~4-6GB** | **~1-2%** âœ… |

**Quantization 4-bit** giáº£m model tá»« 6-7GB xuá»‘ng ~3-4GB khi load!

### Cáº¥u hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u

File `train/Uni-MuMER-train-local.yaml`:
- âœ… `finetuning_type: lora`
- âœ… `quantization_bit: 4` (BitsAndBytes)
- âœ… `lora_rank: 8`, `lora_alpha: 16`
- âœ… `per_device_train_batch_size: 2`
- âœ… `gradient_accumulation_steps: 8`
- âœ… `compute_hmer_accuracy: true` (hiá»ƒn thá»‹ accuracy trong terminal)

### Cháº¡y Training

```bash
cd "/home/nhat/Uni-MuMER-project/BaseLine"
conda activate unimumer
bash train_local.sh
```

---

## ğŸ¯ Train Chá»‰ vá»›i CROHME

Äá»ƒ train chá»‰ vá»›i dataset CROHME (so sÃ¡nh vá»›i bÃ i bÃ¡o):

### ThÃ´ng tin Dataset

- **CROHME 2014**: 986 samples
- **CROHME 2016**: 1147 samples  
- **CROHME 2019**: 1199 samples
- **Tá»•ng cá»™ng**: ~3,332 samples

### Config Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t

File `train/Uni-MuMER-train-local.yaml`:
- âœ… `dataset: crohme_2014, crohme_2016, crohme_2019`
- âœ… `output_dir: .../uni-mumer_crohme_local`
- âœ… `num_train_epochs: 5.0` (test trÃªn local)

### Thá»i gian train Æ°á»›c tÃ­nh

- **Local (RTX 3070)**: 5 epochs â‰ˆ 30-60 phÃºt
- **Kaggle (2x T4)**: 100 epochs â‰ˆ 8 giá»

---

## âš¡ Tá»‘i Æ¯u Training

### Má»¥c tiÃªu: 100 Epochs trong 8 giá» (Kaggle)

### CÃ¡c tá»‘i Æ°u Ä‘Ã£ Ã¡p dá»¥ng

1. **Giáº£m Image Resolution**: `image_max_pixels: 32768` (giáº£m 50%)
2. **Giáº£m LoRA Rank**: `lora_rank: 8` (giáº£m 50% tham sá»‘)
3. **Giáº£m Sequence Length**: `cutoff_len: 1024` (giáº£m 50%)
4. **Giáº£m Workers**: Giáº£m overhead I/O
5. **TÄƒng Batch Size**: Táº­n dá»¥ng VRAM (Kaggle: 4 per GPU Ã— 2 GPU)

### TÃ­nh toÃ¡n thá»i gian

**Kaggle (2x T4)**:
- Effective batch: 32
- Steps/epoch: ~104
- 100 epochs: 10,400 steps
- Cáº§n: **2.77 giÃ¢y/step** âœ… (Kháº£ thi!)

**Local (RTX 3070)**:
- Effective batch: 16
- Steps/epoch: ~208
- 5 epochs: 1,040 steps
- Thá»i gian: **30-60 phÃºt**

### Config Files

- **Local**: `train/Uni-MuMER-train-local.yaml` (test 5 epochs)
- **Kaggle**: `train/Uni-MuMER-train-kaggle.yaml` (100 epochs)

---

## ğŸ“Š Hiá»ƒn Thá»‹ Káº¿t Quáº£ Epoch

### Tá»± Ä‘á»™ng hiá»ƒn thá»‹ trong terminal

Sau má»—i epoch evaluation, báº¡n sáº½ tháº¥y:

```
================================================================================
                            EPOCH 1.00 SUMMARY                             
================================================================================

ğŸ“Š Training Metrics:
  â€¢ Loss:              0.0573
  â€¢ Learning Rate:     0.000381692

ğŸ“ˆ Evaluation Metrics:
  â€¢ Eval Loss:         0.0450
  â€¢ Eval Perplexity:   1.0460

ğŸ¯ HMER Accuracy:
  â€¢ Exact Match Rate:  72.34% (0.7234)
  â€¢ Avg Edit Distance: 0.1234

â±ï¸  Progress: 19.1% (208/1045 steps)
================================================================================
```

### Xem káº¿t quáº£ tá»«ng epoch

```bash
# Tá»•ng há»£p táº¥t cáº£ epochs
python scripts/summarize_epoch_results.py saves/.../uni-mumer_crohme_local

# Xem káº¿t quáº£ epoch cá»¥ thá»ƒ
cat saves/.../checkpoint-3/eval_results.json | python -m json.tool
```

### Cáº¥u trÃºc files sau training

```
saves/.../uni-mumer_crohme_local/
â”œâ”€â”€ checkpoint-1/
â”‚   â””â”€â”€ eval_results.json  # â† Káº¿t quáº£ epoch 1
â”œâ”€â”€ checkpoint-2/
â”‚   â””â”€â”€ eval_results.json  # â† Káº¿t quáº£ epoch 2
â”œâ”€â”€ trainer_state.json     # â† Tá»•ng há»£p
â””â”€â”€ epoch_results_summary.csv  # â† Tá»•ng há»£p (sau khi cháº¡y script)
```

---

## ğŸš€ Train trÃªn Kaggle

### BÆ°á»›c 1: Push code lÃªn GitHub

```bash
cd "/home/nhat/Uni-MuMER-project"
git add .
git commit -m "Uni-MuMER training setup"
git push
```

### BÆ°á»›c 2: Táº¡o Kaggle Notebook

1. ÄÄƒng nháº­p [Kaggle](https://www.kaggle.com/)
2. VÃ o **Notebooks** â†’ **New Notebook**
3. Chá»n **2x T4 GPU** accelerator

### BÆ°á»›c 3: Cháº¡y Training

Sá»­ dá»¥ng notebook `UniMER_Kaggle_Setup.ipynb` hoáº·c cháº¡y trá»±c tiáº¿p:

```bash
bash train_kaggle.sh
```

Hoáº·c trong Kaggle notebook:

```python
!bash train_kaggle.sh
```

---

## ğŸ”§ Troubleshooting

### Lá»—i OOM (Out of Memory)

1. Giáº£m `per_device_train_batch_size` xuá»‘ng 1
2. TÄƒng `gradient_accumulation_steps`
3. Giáº£m `image_max_pixels` xuá»‘ng 16384
4. Giáº£m `lora_rank` xuá»‘ng 4

### Lá»—i CUDA

```bash
# Kiá»ƒm tra CUDA version
nvidia-smi
python -c "import torch; print(torch.version.cuda)"

# CÃ i láº¡i PyTorch náº¿u cáº§n
conda activate unimumer
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

### Training quÃ¡ cháº­m

1. Giáº£m `preprocessing_num_workers` vÃ  `dataloader_num_workers`
2. TÄƒng `per_device_train_batch_size` náº¿u cÃ²n VRAM
3. Giáº£m `image_max_pixels`

---

## ğŸ“ LÆ°u Ã Quan Trá»ng

1. **LuÃ´n kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda**:
   ```bash
   conda activate unimumer
   ```

2. **Monitor GPU usage**:
   ```bash
   watch -n 1 nvidia-smi
   ```

3. **LÆ°u checkpoints thÆ°á»ng xuyÃªn** Ä‘á»ƒ cÃ³ thá»ƒ resume

4. **KhÃ´ng push dá»¯ liá»‡u vÃ  checkpoints** lÃªn GitHub (quÃ¡ lá»›n)

---

## ğŸ¯ TÃ³m Táº¯t Workflow

```
Local Machine (RTX 3070)
â”œâ”€â”€ Setup conda environment
â”œâ”€â”€ Train vÃ i epoch (5 epochs) Ä‘á»ƒ test
â””â”€â”€ Push code lÃªn GitHub
    â”‚
    â””â”€â”€> Kaggle (2x T4)
        â”œâ”€â”€ Clone tá»« GitHub
        â”œâ”€â”€ Train 100 epochs (8 giá»)
        â””â”€â”€ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
```

---

**Xem thÃªm**: `HUONG_DAN_ACCURACY.md` Ä‘á»ƒ biáº¿t cÃ¡ch theo dÃµi accuracy chi tiáº¿t.
