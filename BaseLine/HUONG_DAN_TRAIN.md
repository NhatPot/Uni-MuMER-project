# ğŸ“š HÆ°á»›ng Dáº«n Train Uni-MuMER

HÆ°á»›ng dáº«n chi tiáº¿t Ä‘á»ƒ train mÃ´ hÃ¬nh Uni-MuMER trÃªn mÃ¡y local vÃ  sau Ä‘Ã³ tiáº¿p tá»¥c trÃªn Kaggle.

## ğŸ–¥ï¸ Cáº¥u HÃ¬nh MÃ¡y Cá»§a Báº¡n

- **CPU**: Intel E5-2680v4
- **GPU**: NVIDIA RTX 3070 (8GB VRAM)
- **RAM**: 144GB
- **OS**: Linux

## ğŸ“‹ BÆ°á»›c 1: Setup MÃ´i TrÆ°á»ng Conda (Local)

### CÃ¡ch 1: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

```bash
cd "/home/nhat/Uni-MuMER-project/BaseLine"
bash setup_conda_local.sh
```

Script nÃ y sáº½:
- Táº¡o mÃ´i trÆ°á»ng conda `unimumer` vá»›i Python 3.10
- CÃ i Ä‘áº·t PyTorch vá»›i CUDA 12.1
- CÃ i Ä‘áº·t táº¥t cáº£ dependencies tá»« `requirements_training.txt`
- Clone vÃ  cÃ i Ä‘áº·t LLaMA-Factory
- Táº£i NLTK data

### CÃ¡ch 2: Thá»§ cÃ´ng

```bash
# 1. Táº¡o mÃ´i trÆ°á»ng
conda create -n unimumer python=3.10 -y

# 2. KÃ­ch hoáº¡t
conda activate unimumer

# 3. CÃ i PyTorch
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# 4. Upgrade pip
pip install --upgrade pip

# 5. CÃ i dependencies
cd "/home/nhat/Uni-MuMER-project/BaseLine"
pip install -r requirements_training.txt

# 6. Clone LLaMA-Factory
mkdir -p train
cd train
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .
cd ../..
```

## ğŸ“¦ BÆ°á»›c 2: Chuáº©n Bá»‹ Dá»¯ Liá»‡u

1. **Giáº£i nÃ©n dá»¯ liá»‡u** (náº¿u chÆ°a cÃ³):
   ```bash
   cd "/home/nhat/Uni-MuMER-project/BaseLine"
   unzip data.zip -d .
   ```

2. **Kiá»ƒm tra cáº¥u trÃºc dá»¯ liá»‡u**:
   ```
   data/
   â”œâ”€â”€ CROHME/
   â”œâ”€â”€ CROHME2023/
   â”œâ”€â”€ HME100K/
   â”œâ”€â”€ Im2LaTeXv2/
   â”œâ”€â”€ MathWriting/
   â””â”€â”€ MNE/
   ```

## ğŸ‹ï¸ BÆ°á»›c 3: Train VÃ i Epoch TrÃªn Local

### Cháº¡y training:

```bash
cd "/home/nhat/Uni-MuMER-project/BaseLine"
conda activate unimumer
bash train_local.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:

```bash
conda activate unimumer
cd train/LLaMA-Factory
llamafactory-cli train ../Uni-MuMER-train-local.yaml
```

### Cáº¥u hÃ¬nh cho RTX 3070:

File `train/Uni-MuMER-train-local.yaml` Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u cho RTX 3070:
- `per_device_train_batch_size: 1` (nhá» Ä‘á»ƒ trÃ¡nh OOM)
- `gradient_accumulation_steps: 16` (effective batch size = 16)
- `num_train_epochs: 2.0` (train vÃ i epoch)
- KhÃ´ng dÃ¹ng DeepSpeed (khÃ´ng cáº§n cho single GPU)

### Monitor training:

Training logs sáº½ Ä‘Æ°á»£c lÆ°u táº¡i:
- TensorBoard: `train/LLaMA-Factory/saves/qwen2.5_vl-3b/full/sft/standred/uni-mumer_local/`
- Checkpoints: TÆ°Æ¡ng tá»±, má»—i 500 steps

Xem TensorBoard:
```bash
conda activate unimumer
cd train/LLaMA-Factory
tensorboard --logdir saves/qwen2.5_vl-3b/full/sft/standred/uni-mumer_local
```

## ğŸ“¤ BÆ°á»›c 4: Push LÃªn GitHub

### 4.1. Khá»Ÿi táº¡o Git (náº¿u chÆ°a cÃ³)

```bash
cd "/home/nhat/Uni-MuMER-project"
git init
git add .
git commit -m "Initial commit: Uni-MuMER training setup"
```

### 4.2. Táº¡o repository trÃªn GitHub

1. ÄÄƒng nháº­p GitHub
2. Táº¡o repository má»›i (vÃ­ dá»¥: `Nhan-Dien-Ky-Tu-Toan-Hoc`)
3. **KHÃ”NG** khá»Ÿi táº¡o vá»›i README, .gitignore, hoáº·c license

### 4.3. Push code lÃªn GitHub

```bash
# ThÃªm remote (thay YOUR_USERNAME báº±ng username cá»§a báº¡n)
git remote add origin https://github.com/YOUR_USERNAME/Nhan-Dien-Ky-Tu-Toan-Hoc.git

# Push code
git branch -M main
git push -u origin main
```

### 4.4. Táº¡o .gitignore (náº¿u chÆ°a cÃ³)

Táº¡o file `.gitignore` trong thÆ° má»¥c gá»‘c:

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
ENV/
.venv

# Conda
.conda/

# Data
data/
*.zip
*.tar.gz

# Model checkpoints
saves/
checkpoints/
*.pt
*.pth
*.bin
*.safetensors

# Logs
logs/
lightning_logs/
tensorboard_logs/
*.log

# Jupyter
.ipynb_checkpoints/
*.ipynb_checkpoints

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# LLaMA-Factory
train/LLaMA-Factory/
```

**LÆ°u Ã½**: KhÃ´ng push dá»¯ liá»‡u vÃ  checkpoints lÃªn GitHub (quÃ¡ lá»›n). Chá»‰ push code vÃ  config.

## ğŸš€ BÆ°á»›c 5: Train TrÃªn Kaggle

### 5.1. Táº¡o Kaggle Notebook

1. ÄÄƒng nháº­p [Kaggle](https://www.kaggle.com/)
2. VÃ o **Notebooks** â†’ **New Notebook**
3. Chá»n **GPU** accelerator (P100 hoáº·c T4 trá»Ÿ lÃªn)

### 5.2. Sá»­ dá»¥ng Notebook Setup

1. **Upload notebook** `UniMER_Kaggle_Setup.ipynb` lÃªn Kaggle
2. Hoáº·c **táº¡o notebook má»›i** vÃ  copy ná»™i dung tá»« `UniMER_Kaggle_Setup.ipynb`

### 5.3. Chá»‰nh sá»­a notebook

Trong cell clone repository, thay `YOUR_USERNAME` báº±ng username GitHub cá»§a báº¡n:

```python
# Clone repo
!git clone https://github.com/YOUR_USERNAME/Nhan-Dien-Ky-Tu-Toan-Hoc.git
```

### 5.4. Cháº¡y notebook

1. Cháº¡y tá»«ng cell theo thá»© tá»±
2. Notebook sáº½:
   - CÃ i Ä‘áº·t Miniconda
   - Táº¡o mÃ´i trÆ°á»ng Python 3.10
   - Clone repository tá»« GitHub
   - CÃ i Ä‘áº·t dependencies
   - Cháº¡y training

### 5.5. Resume tá»« checkpoint (náº¿u cáº§n)

Náº¿u báº¡n Ä‘Ã£ train vÃ i epoch trÃªn local, cÃ³ thá»ƒ upload checkpoint lÃªn Kaggle dataset vÃ  resume:

```python
# Trong notebook, sau khi clone repo
# Copy checkpoint tá»« Kaggle dataset
!cp -r /kaggle/input/unimer-checkpoints/checkpoint-XXXX /kaggle/working/Nhan-Dien-Ky-Tu-Toan-Hoc/BaseLine/train/LLaMA-Factory/saves/...

# Sau Ä‘Ã³ resume training
# Sá»­a Uni-MuMER-train.yaml: resume_from_checkpoint: saves/.../checkpoint-XXXX
```

## ğŸ“Š BÆ°á»›c 6: ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh

Sau khi training xong, Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh:

```bash
# TrÃªn Kaggle hoáº·c local
conda activate unimumer
cd "/home/nhat/Uni-MuMER-project/BaseLine"

# ÄÃ¡nh giÃ¡ trÃªn táº¥t cáº£ test sets
bash eval/eval_all.sh -m saves/qwen2.5_vl-3b/full/sft/standred/uni-mumer_full -s test1 -b 32768

# Hoáº·c Ä‘Ã¡nh giÃ¡ tá»«ng dataset
bash eval/eval_crohme.sh -m saves/... -b 32768
bash eval/eval_hme100k.sh -m saves/... -b 32768
```

## ğŸ”§ Troubleshooting

### Lá»—i OOM (Out of Memory)

Náº¿u gáº·p lá»—i OOM trÃªn RTX 3070:
1. Giáº£m `per_device_train_batch_size` xuá»‘ng 1
2. TÄƒng `gradient_accumulation_steps`
3. Giáº£m `image_max_pixels` trong config
4. Sá»­ dá»¥ng gradient checkpointing (náº¿u LLaMA-Factory há»— trá»£)

### Lá»—i CUDA

```bash
# Kiá»ƒm tra CUDA version
nvidia-smi
python -c "import torch; print(torch.version.cuda)"

# Náº¿u khÃ´ng khá»›p, cÃ i láº¡i PyTorch vá»›i Ä‘Ãºng CUDA version
conda activate unimumer
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y
```

### Lá»—i khi clone LLaMA-Factory

```bash
# XÃ³a vÃ  clone láº¡i
rm -rf train/LLaMA-Factory
cd train
git clone https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e .
```

## ğŸ“ LÆ°u Ã Quan Trá»ng

1. **LuÃ´n kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda** trÆ°á»›c khi cháº¡y:
   ```bash
   conda activate unimumer
   ```

2. **KhÃ´ng push dá»¯ liá»‡u vÃ  checkpoints** lÃªn GitHub (quÃ¡ lá»›n)

3. **LÆ°u checkpoints thÆ°á»ng xuyÃªn** Ä‘á»ƒ cÃ³ thá»ƒ resume

4. **Monitor GPU usage**:
   ```bash
   watch -n 1 nvidia-smi
   ```

5. **Training trÃªn RTX 3070 sáº½ cháº­m hÆ¡n** so vá»›i A100 trÃªn Kaggle, nhÆ°ng váº«n cÃ³ thá»ƒ train Ä‘Æ°á»£c

## ğŸ¯ TÃ³m Táº¯t Workflow

```
Local Machine (RTX 3070)
â”œâ”€â”€ Setup conda environment
â”œâ”€â”€ Train vÃ i epoch (2-3 epochs)
â””â”€â”€ Push code lÃªn GitHub
    â”‚
    â””â”€â”€> Kaggle
        â”œâ”€â”€ Clone tá»« GitHub
        â”œâ”€â”€ Resume tá»« checkpoint (náº¿u cÃ³)
        â”œâ”€â”€ Train tiáº¿p Ä‘áº¿n khi hoÃ n chá»‰nh
        â””â”€â”€ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
```

## ğŸ“ Há»— Trá»£

Náº¿u gáº·p váº¥n Ä‘á», kiá»ƒm tra:
- Logs trong `train/LLaMA-Factory/saves/.../`
- TensorBoard logs
- GPU memory usage vá»›i `nvidia-smi`

