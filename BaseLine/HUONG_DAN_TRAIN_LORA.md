# HÆ°á»›ng dáº«n Training vá»›i LoRA + Quantization 4-bit

## âœ… ÄÃ£ cáº¥u hÃ¬nh xong!

Config Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn tá»« **Full Fine-tuning** sang **LoRA + Quantization 4-bit** Ä‘á»ƒ tiáº¿t kiá»‡m memory tá»‘i Ä‘a cho GPU 8GB.

## ğŸ“Š So sÃ¡nh

| PhÆ°Æ¡ng phÃ¡p | Memory cáº§n | Tham sá»‘ train |
|------------|------------|---------------|
| Full Fine-tuning | ~25GB | 100% (3B params) |
| LoRA | ~8-10GB | ~1-2% (vÃ i triá»‡u params) |
| **LoRA + 4-bit Quantization** | **~4-6GB** | **~1-2% (vÃ i triá»‡u params)** âœ… |

**Quantization 4-bit** giáº£m model tá»« 6-7GB xuá»‘ng ~3-4GB khi load!

## ğŸš€ CÃ¡ch cháº¡y

### BÆ°á»›c 1: Äáº£m báº£o mÃ´i trÆ°á»ng Ä‘Ã£ sáºµn sÃ ng

```bash
cd /home/nhat/Uni-MuMER-project/BaseLine
conda activate unimumer
```

### BÆ°á»›c 2: Kiá»ƒm tra GPU memory

```bash
nvidia-smi
```

### BÆ°á»›c 3: Cháº¡y training

```bash
bash train_local.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:

```bash
cd train/LLaMA-Factory
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
llamafactory-cli train ../Uni-MuMER-train-local.yaml
```

## ğŸ“ CÃ¡c thay Ä‘á»•i Ä‘Ã£ thá»±c hiá»‡n

1. âœ… `finetuning_type: lora` - Chuyá»ƒn sang LoRA
2. âœ… `quantization_bit: 4` - **QUAN TRá»ŒNG**: Quantization 4-bit Ä‘á»ƒ giáº£m memory khi load model
3. âœ… `quantization_method: bnb` - Sá»­ dá»¥ng BitsAndBytes
4. âœ… `double_quantization: true` - Tiáº¿t kiá»‡m thÃªm memory
5. âœ… `lora_rank: 16` - Rank cá»§a LoRA adapter
6. âœ… `lora_alpha: 32` - Scaling factor
7. âœ… `per_device_train_batch_size: 1` - Giá»¯ = 1 Ä‘á»ƒ an toÃ n
8. âœ… `learning_rate: 5.0e-4` - TÄƒng LR cho LoRA (thÆ°á»ng cao hÆ¡n full fine-tuning)
9. âœ… `image_max_pixels: 65536` - Giáº£m Ä‘á»ƒ Ä‘áº£m báº£o Ä‘á»§ memory
10. âœ… ÄÃ£ cÃ i `bitsandbytes` package

## ğŸ“‚ Output

Checkpoints sáº½ Ä‘Æ°á»£c lÆ°u táº¡i:
```
saves/qwen2.5_vl-3b/lora/sft/standred/uni-mumer_local/
```

## ğŸ” Kiá»ƒm tra tiáº¿n trÃ¬nh

Training sáº½ hiá»ƒn thá»‹:
- Loss giáº£m dáº§n
- Memory usage (nÃªn tháº¥p hÆ¡n nhiá»u so vá»›i full fine-tuning)
- Checkpoints Ä‘Æ°á»£c lÆ°u má»—i 500 steps

## âš ï¸ Náº¿u váº«n bá»‹ OOM

Náº¿u váº«n gáº·p lá»—i Out of Memory, thá»­:

1. Giáº£m `per_device_train_batch_size` xuá»‘ng `1`
2. Giáº£m `image_max_pixels` xuá»‘ng `65536`
3. Giáº£m `lora_rank` xuá»‘ng `8`

## ğŸ“š TÃ i liá»‡u thÃªm

- LoRA paper: https://arxiv.org/abs/2106.09685
- LLaMA-Factory docs: https://github.com/hiyouga/LLaMA-Factory

