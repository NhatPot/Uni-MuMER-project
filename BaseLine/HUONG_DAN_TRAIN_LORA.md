# HÆ°á»›ng dáº«n Training vá»›i LoRA

## âœ… ÄÃ£ cáº¥u hÃ¬nh xong!

Config Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn tá»« **Full Fine-tuning** sang **LoRA** Ä‘á»ƒ tiáº¿t kiá»‡m memory cho GPU 8GB.

## ğŸ“Š So sÃ¡nh

| PhÆ°Æ¡ng phÃ¡p | Memory cáº§n | Tham sá»‘ train |
|------------|------------|---------------|
| Full Fine-tuning | ~25GB | 100% (3B params) |
| **LoRA** | **~8-10GB** | **~1-2% (vÃ i triá»‡u params)** |

## ğŸš€ CÃ¡ch cháº¡y

### BÆ°á»›c 1: Äáº£m báº£o mÃ´i trÆ°á»ng Ä‘Ã£ sáºµn sÃ ng

```bash
cd /home/nhat/Uni-MuMER-project/BaseLine
conda activate unimumer
```

### BÆ°á»›c 2: Kiá»ƒm tra GPU memory

```bash
nvidia-smi
```

### BÆ°á»›c 3: Cháº¡y training

```bash
bash train_local.sh
```

Hoáº·c cháº¡y trá»±c tiáº¿p:

```bash
cd train/LLaMA-Factory
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
llamafactory-cli train ../Uni-MuMER-train-local.yaml
```

## ğŸ“ CÃ¡c thay Ä‘á»•i Ä‘Ã£ thá»±c hiá»‡n

1. âœ… `finetuning_type: lora` - Chuyá»ƒn sang LoRA
2. âœ… `lora_rank: 16` - Rank cá»§a LoRA adapter
3. âœ… `lora_alpha: 32` - Scaling factor
4. âœ… `per_device_train_batch_size: 2` - TÄƒng batch size (LoRA tiáº¿t kiá»‡m memory)
5. âœ… `learning_rate: 5.0e-4` - TÄƒng LR cho LoRA (thÆ°á»ng cao hÆ¡n full fine-tuning)
6. âœ… `image_max_pixels: 131072` - TÄƒng láº¡i vÃ¬ LoRA tiáº¿t kiá»‡m memory

## ğŸ“‚ Output

Checkpoints sáº½ Ä‘Æ°á»£c lÆ°u táº¡i:
```
saves/qwen2.5_vl-3b/lora/sft/standred/uni-mumer_local/
```

## ğŸ” Kiá»ƒm tra tiáº¿n trÃ¬nh

Training sáº½ hiá»ƒn thá»‹:
- Loss giáº£m dáº§n
- Memory usage (nÃªn tháº¥p hÆ¡n nhiá»u so vá»›i full fine-tuning)
- Checkpoints Ä‘Æ°á»£c lÆ°u má»—i 500 steps

## âš ï¸ Náº¿u váº«n bá»‹ OOM

Náº¿u váº«n gáº·p lá»—i Out of Memory, thá»­:

1. Giáº£m `per_device_train_batch_size` xuá»‘ng `1`
2. Giáº£m `image_max_pixels` xuá»‘ng `65536`
3. Giáº£m `lora_rank` xuá»‘ng `8`

## ğŸ“š TÃ i liá»‡u thÃªm

- LoRA paper: https://arxiv.org/abs/2106.09685
- LLaMA-Factory docs: https://github.com/hiyouga/LLaMA-Factory

